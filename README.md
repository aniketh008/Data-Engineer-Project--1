ğŸš€ Project 1 â€“ Enterprise-Grade Customer Account Data Pipeline (Azure)
ğŸ“Œ Overview

This project demonstrates the design and implementation of a production-ready Azure data pipeline built using modern cloud data engineering principles.

The solution ingests raw banking datasets, processes them through a multi-layered architecture (Bronze â†’ Silver â†’ Gold), and delivers curated data into Azure SQL Database for analytics and reporting.

Designed with scalability, modularity, and enterprise best practices in mind.

ğŸ— Architecture Overview
ETL Workflow

The architecture follows the Medallion Design Pattern:

Bronze Layer (Raw) â€“ Immutable raw ingestion

Silver Layer (Cleaned) â€“ Standardized & validated datasets

Gold Layer (Curated) â€“ Business-ready structured data

ğŸ¯ Business Objective

Build a scalable Azure-based ETL framework to:

Ingest customer banking datasets from backend storage

Perform cleansing and schema standardization

Apply business transformations including SCD logic

Load curated data into Azure SQL Database

Enable downstream reporting in Power BI

ğŸ”„ End-to-End Data Engineering Workflow
ğŸŸ¤ Step 1: Data Ingestion (Source â†’ Bronze Layer)

Source

Local File System

Files:

accounts.csv

customers.csv

loan_payments.csv

loans.csv

transactions.csv

Dataset Reference
AI Bank Dataset (Kaggle)

Process

Built parameterized ADF pipelines

Ingested raw files into Azure Data Lake Storage Gen2 (Raw/Bronze container)

Enabled metadata-driven ingestion for scalability

Design Considerations

Schema drift handling

Re-runnable pipeline logic

Folder-based partitioning strategy

âšª Step 2: Data Cleansing (Bronze â†’ Silver Layer)

Implemented transformation logic using Azure Data Factory Mapping Data Flows:

Removed duplicate records

Handled nulls and missing attributes

Applied consistent schema and data type conversions

Standardized column naming conventions

Generated optimized Parquet/Delta outputs

Silver layer acts as the trusted, validated data zone.

ğŸŸ¡ Step 3: Data Transformation (Silver â†’ Gold Layer)

This layer applies business logic and prepares dimensional models.

Key Implementations:

SCD Type 1 (Overwrite logic for non-historical changes)

SCD Type 2 (History tracking with effective & expiry dates)

Surrogate key generation

Fact and dimension table modeling

Final curated data is loaded into:

â¡ Azure SQL Database

Designed for:

Optimized query performance

BI consumption

Structured relational analytics

ğŸ” Pipeline Orchestration

Three modular pipelines were developed:

Local â†’ Bronze

Bronze â†’ Silver

Silver â†’ Gold

Features include:

Parameterized datasets

Pipeline triggers (scheduled execution)

Dependency chaining

Failure handling & monitoring

Secure secret management via Azure Key Vault

ğŸ“Š Data Visualization (Power BI)

Connected Power BI to Azure SQL Database

Designed business KPIs for:

Customer analysis

Loan performance

Transaction insights

Published dashboards to Microsoft Fabric Workspace

âš™ï¸ Key Engineering Highlights

âœ” Medallion architecture implementation

âœ” Dynamic and reusable ADF pipelines

âœ” Secure credential management using Azure Key Vault

âœ” Incremental load ready design

âœ” Optimized storage format (Parquet/Delta)

âœ” Modular and scalable framework

ğŸ›  Technology Stack

Azure Data Factory

Azure Data Lake Storage Gen2

Azure SQL Database

Azure Key Vault

Power BI

Delta / Parquet format

ğŸ’¡ What This Project Demonstrates

As an Azure Data Engineer with 4+ years of experience, this project reflects:

Strong understanding of cloud-native data architecture

Experience building enterprise ETL frameworks

Hands-on implementation of SCD logic

Data modeling and BI integration capabilities

Production-grade orchestration and security practices
